\chapter{Discussions}\label{chapter:discussions}

\section{Performance Findings}
\subsection{Comparision of Production Throughputs}
  As seen in \autoref{fig:result-productionIsolate} and \autoref{fig:result-productionMessageSize}, the difference seen between throughput at a worker isolate and throughput at RabbitMQ was quite significant. This is because the production of messages in an isolate were localized to one particular instance before the messages were transferred to the MQS over a WebSocket. Thus, it is natural the message production of isolates were high. Message production throughput increased with increasing the number of producers, but average throughput per isolate declined. One possible explanation for this behavior could be a bottleneck at the MQS to which the producers send messages. This explanation might seem strange because sending messages in an isolate are asynchronous, so it should not have been affected by an outside decoupled component. But, the reason behind this speculation is that since the MQS cannot accept messages at the rate they are produced, the messages get buffered in the internal queues of top level isolates. This takes up more heap memory reserved for the isolate causing slower execution. Similar effects probably occur in controller isolates, then in router isolates and ultimately in worker isolates. Hence, as the available heap memory of worker isolates becomes lower the production throughput decreases.

\subsection{Effect of Message Size}
  From \autoref{fig:result-consumptionMessageSize} and \autoref{fig:result-productionMessageSize}, we observed the negative impact of increase in message size. If we assess the amount by which the performance decreased, we can see that doubling the message size did not cause halving the performance. Thus, from this finding we can infer that instead of two separate chunks of messages, if we send messages as a big chunk the overall throughput increases. For instance, let's consider the results of 64-byte messages and 1024-byte messages. The throughput of 64-byte messages was 8,400 messages per second whereas that of 1024-byte messages was 3,097 messages per second. If we assume that the 1024-byte message was the concatenation of sixteen 64-byte messages, then the overall throughput would become 49,552 messages per second, which is an overall improvement of about 6 times. This could mean that for small messages, time is spent more on processing for persistence at the message broker rather than on Input/Output of messages.

  A similar case for production throughput can be seen in \autoref{fig:result-productionMessageSize}. With similar calculations for 64-byte messages and 1024-byte messages, if the messages are chunked together, we can gain approximately 13 times greater throughput.

  Nevertheless, such concatenation may not always be feasible and may prove to be against the fair distribution of messages across systems. However, if it is appropriate to concat multiple messages in certain systems, then finding an optimum size (by considering the other requirements of the application) for a message to maximize throughput might prove to be quite beneficial.

\subsection{Effect of Scaling out the Message Queuing System}
  Scaling out the MQS clearly had positive impact on overall throughput. Higher number of MQS instances supported more consumers with good overall performance. The decline of performance seen in tests with 1 MQS instance and 2 MQS instances were probably because of the behavior of RabbitMQ when too many consumers are connected as we discuss in~\autoref{subsec:discussion-prefetch}. But, the test seen with four MQS instances suggests otherwise. With four MQS instances, 32 consumers were supported without a drop in performance. From which, we can speculate that the MQS may be causing the limit in throughput, as the MQS has to distribute messages to the connected systems. As more consumers are connected to the MQS, more CPU time and memory are required.


\subsection{Effect of Prefetch Count}
\label{subsec:discussion-prefetch}
  In the observations made in \autoref{fig:result-prefetch}, throughput scaled almost linearly to 8 consumers for every prefetch-count (except when the prefetch-count was 16). Adding more consumers after 8 resulted in a decrease of throughput, which is quite similar to the RabbitMQ benchmark~[\autoref{fig:rabbitmqBenchmark}] presented in ~\autoref{subsec:rabbitmqPrefetch}. In the benchmark of RabbitMQ, a decline of throughput was seen with 8 to 10 consumers.

  The greater the number of consumers, the more work is required by RabbitMQ to keep track of them. Hence, it could be the same reason we saw the decline in performance with more consumers in \autoref{fig:result-prefetch}.

  In a distributed system, a sudden performance change could be due to a number of factors, such as network speed, the system in which the isolate system is running or because of a bottleneck in the MQS. But each time the performance seems to drop after around 8 consumers. The number of consumers after which the decline in performance occurred in \autoref{subsec:rabbitmqPrefetch} and \autoref{fig:result-prefetch} suggests that the reason could be due to the broker handling too many consumers for a queue.

\subsection{Effect of Simultaneous Production and Consumption}
In \autoref{fig:result-simultaneous1}, we can see the negative impact of starting producers along with consumers when the producers were producing messages continuously. Again, when the producers and consumers were split to connect to separate instances of the MQS, the performance improvement was more prominent in message consumption.

  By comparing \autoref{fig:result-simultaneous1} and \autoref{fig:result-simultaneous2}, the culprit of this behavior seems to be the MQS. Even though the MQS has separate isolates for enqueuing and dequeuing, it has only one top level isolate~[\autoref{subsec:mqs}]. Obviously when there is a large quantity of messages from a producer in queue, the dequeue requests that arrive from the consumer and the messages that arrive from the Dequeuer go further back in the internal isolate queue. The improvement of consumption throughput seen in
   \autoref{fig:result-simultaneous2} also supports this explanation because in this case, the MQS where consumers are connected does not have to deal with the large surge of production messages. Thus, the dequeue requests and dequeued messages from the dequeuer get processed much quicker compared to the previous case.

\subsection{Throughput of Request-Reply}
  The benchmark of request-reply~[\autoref{subsec:request-reply}] showed almost linear improvement with increase in the number of requesters. But, increasing the number of suppliers from 1 to 8 had little effect in overall performance. The reason behind this could be the design of the program which uses the \emph{ask} method to send messages. Based on how \emph{ask} works as discussed in \autoref{subsec:implementationOverview}, even a single supplier might not have been saturated with the number of requesters it could handle. A slight increase was seen with more suppliers, but this is quite minimal and could have been affected by other factors like RabbitMQ performance, change in network latency, etc. There is not enough data and evidence to support improvements in throughput with more suppliers. Testing with more consumers till the supplier saturates would yield a better understanding of what affects throughput in the request-reply case.

\subsection{Round Trip Time of Request-Reply}
  The observation seen in \autoref{fig:result-request-reply-rtt} was inconsistent. It is difficult to make any strong conclusions from the data. Nevertheless, it seems that the increase in number of suppliers decreases latency, as seen in the case of eight suppliers against a single supplier.

  There could be many factors affecting the result. Even a slight change in network latency can induce a significant change in time required to transfer a message.

\section{The DDE Framework}
  According to the observations made after creating and testing the applications based on the framework (as discussed in \autoref{subsec:result-framework}), we can infer the applications responded well when the system was scaled up. We can see a rise in message throughput when additional isolate systems are added. The rise in message throughput was even more when the number of MQS instances were increased.

  During testing, restarting the message broker system (RabbitMQ) and restarting the message queuing systems did not result in failure of the whole system. The system operated normally as soon as they were back online. Thus, we can say that the system was highly available and it is suitable for systems that must be “run forever”.

\section{Problems/Issues}

\subsection{Dart Induced Issues}
\label{subsec:result-dartIssues}
  Even though Dart is a fairly mature language, it still has some bugs and incomplete features. Some of the issues could be solved by workarounds while others could not be easily resolved. All issues listed here were faced in Dart version 1.7.2.

  Dart's documentation for isolate~\cite{dartApiIsolate} mentions that there is experimental support for the following methods in isolates: \emph{addErrorListener},  \emph{addOnExitListener}, \emph{kill}, \emph{pause}, \emph{ping}, \emph{removeErrorListener}, \emph{removeOnExitListener} and \emph{resume}. But while developing the framework, it was found that these methods were not implemented. The implementation of these functionalities would have particularly helped in the implementation of the supervision strategy. Moreover, the implementation of features like ‘hot deployment’, ‘migration’ and isolate termination would have been cleaner and simpler compared to the current workarounds present in the DDE framework.

  We have already discussed the workaround implementations for \emph{kill} in \autoref{itm:killWorking} and \emph{ping} in \autoref{subsubsec:router}.

  Another issue found in the Dart VM for browsers which made it impossible for an isolate system designed in the DDE framework to run was the lack of support for multi-level isolates. The ‘Dartium’ browser~\footnote{a variant of Chromium browser with built in dart VM}, only supports spawning of isolates up to one level deep. i.e. Spawning of an isolate by the spawned isolate was not allowed. The reason behind the lack of this implementation is not clear.

\subsection{STOMP Library for Dart}
  As mentioned in \autoref{subsec:stompForDart}, a third-party library for the STOMP client in Dart was used from Dart's ‘pub’. Although it had good support for STOMP 1.2, it was missing a feature to set the prefetch-count~[\autoref{subsec:rabbitmqPrefetch}] when subscribing as a consumer. Without setting the prefetch-count the system became unusable, especially when there were many messages in the queue.

  As the project was hosted in GitHub\footnote{\url{http://github.com}} and is licensed under Apache 2.0\footnote{\url{https://github.com/rikulo/stomp/blob/master/LICENSE}}, I forked\footnote{make copy of the source code and add custom modifications} the repository and added the functionality so that the STOMP client could support setting the prefetch-count. After making modifications and testing, I created a pull-request\footnote{request to merge my changes into the master branch of the original repository}, which was swiftly merged\footnote{\url{https://github.com/rikulo/stomp/pull/15}} by the original developer and was updated in Dart's pub as a new version.

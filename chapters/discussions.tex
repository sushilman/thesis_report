\chapter{Discussions}\label{chapter:discussions}

\section{Performance Findings}
\subsection{Effect of Prefetch Count}
\label{subsec:discussion-prefetch}
  In the observations made in \autoref{fig:result-prefetch}, the scaling up of throughput scaled up almost linearly up to 8 consumers for every prefetch-count (except when the prefetch-count was 16). Adding more consumers after 8 resulted in the decrease of throughput, which is quite similar to the result of benchmark~[\autoref{fig:rabbitmqBenchmark}] of RabbitMQ described in ~\autoref{subsec:rabbitmqPrefetch}. In the benchmark of RabbitMQ, the decline of throughput were seen when there are around 8 to 10 consumers.

  The greater the number of consumers, the more work required by RabbitMQ to keep track of the consumers. Hence, it could be the same reason we saw the decline in performance with more number of consumers in \autoref{fig:result-prefetch}.

  In a distributed system, the cause of a sudden performance change could be because of number of factors like network speed, the system in which the isolate system is running or because of the bottle necking in a Message Queuing System, which is the gateway of all the messages. But each time the performance seems to drop after around 8 consumers. The number of consumers after which the decline in performance occurred in \autoref{subsec:rabbitmqPrefetch} and \autoref{fig:result-prefetch} suggests that the reason could be because of message broker system requiring to handle too many consumers for a queue.

\subsection{Effect of Message Size}
  From \autoref{fig:result-consumptionMessageSize} and \autoref{fig:result-productionMessageSize} from \autoref{subsec:messageSize}, we can observe the negative impact of increase in message size. Nevertheless, if we assess the amount by which the performance decreased, we can see that doubling the message size did not cause halving of the performance. Thus, from this finding we can infer that instead of two separate chunks of messages, if we send messages as a big chunk the overall throughput increases. For instance, let's consider the results of 64-bytes messages and 1024-bytes messages. The throughput of 64-bytes messages was 8400 whereas that of 1024 was 3097. If we assume that the 1024-Bytes message was the concatenation of sixteen 64-Bytes messages, then the overall throughput would become 49552 messages per second. The overall increment of throughput by about 6 times! This means that for small messages, time is spent more on processing rather than on Input/Output.

  The similar case for production throughput can be seen in \autoref{fig:result-productionMessageSize}. With similar calculations for 64-bytes messages and 1024-bytes messages, if the messages are chunked together, we can gain approximately 13 times greater throughput.

  Nevertheless, such concatenation may not always be feasible and may prove to be against the fair distribution of messages across systems. However, if it is appropriate to concat multiple messages in certain systems, then finding an optimum size (by considering the other requirements of the application) of a message which gives maximizes throughput might prove to be quite beneficial.

\subsection{Effect of Scaling out Message Queuing System}
  The scaling out of MQS clearly had positive impact on overall throughput. Higher number of MQS also allowed to have more consumers with good overall performance. The decline of performance seen in tests with 1 MQS and 2 MQS were probably because of the behavior of RabbitMQ when too many consumers are connected as we discussed in~\autoref{subsec:discussion-prefetch}. But, the test seen with four MQS suggests otherwise. With four MQS, 32 consumers were supported without drop in performance. From which, we can speculate that MQS might also be causing the limit in the throughput as MQS has to distribute messages to the connected systems. The more the consumers connected to MQS, the more the CPU time and memory it requires.

\subsection{Comparision of production throughputs}
  As seen in ~\autoref{fig:result-productionIsolate} and ~\autoref{fig:result-productionMessageSize}, the difference seen between the throughput at a worker isolate and throughput at RabbitMQ is quite significant. This is because the production of message in an isolate is localized to one particular instance before the message is transferred to MQS over WebSocket. Thus, it is natural the production of isolates are that high. Nonetheless, when the overall producers were increased, although overall message production throughput increased, average throughput per isolate declined. One possible explanation to this behavior could be the bottle necking at MQS to which all of these producers send message. The explanation might seem strange because sending messages in isolate are asynchronous. So, it should not have been affected by a outside decoupled component. But, the reason behind this speculation is that since MQS cannot accept messages at the rate which it is produced, the messages get buffered in the internal queues of top level isolates. This takes up more heap memory reserved for the isolate and so it starts to get slower and the similar effect probably occurs in Controller isolate and then in Router isolate and ultimately in Worker isolate, each of them buffering more messages into internal queue. Hence, as the available heap memory of Worker isolate becomes lower the production throughput decreases.

\subsection{Effect of Simultaneous Production and Consumption}
In \autoref{fig:result-simultaneous1}, we can see the negative impact of starting producer along with consumer when producer was producing messages continuously. Again, when the producers and consumers were split to connect to separate instances of MQS, the performance improvement was more prominent in message consumption.
  By comparing \autoref{fig:result-simultaneous1} and \autoref{fig:result-simultaneous2}, the culprit of this behavior seems to be the MQS. The MQS, even though has separate isolates for Enqueuer and Dequeuer, has only one top level isolate~\autoref{subsec:mqs}. Obviously when there is a large quantity of messages from producer in queue, the dequeue requests that arrive from the consumer isolate as well as the the dequeued messages that arrive from Dequeuer goes further back in the internal isolate queue. The improvement of consumption throughput seen in
   \autoref{fig:result-simultaneous2} also supports this explanation because in this case, the MQS where consumers are connected does not have to deal with the large surge of production messages. Thus, the dequeue requests and dequeued messages from dequeuer get processed much quicker compared to previous case.

\subsection{Throughput of Request-Reply}
  The benchmark of request-reply~[\autoref{subsec:request-reply}] showed almost linear increment with increase in number of requesters. But, increase in number of suppliers from 1 to 8 had little effect in overall performance. The reason behind this could be the design of the program which uses \emph{ask} method to send message. Based on how \emph{ask} works as discussed in \autoref{subsec:implementationOverview}, even a single supplier might not have been saturated with the number of requesters it could handle. A slight increase was seen with more supplier, but this is quite minimal and could have been affected by any other factors like RabbitMQ performance, change in network latency, etc. There is not enough data and evidence to support the idea of big improvement in throughput with more suppliers. Testing with more number of consumers till the supplier saturates would yield better result and better idea of what affects throughput of request-reply.

\subsection{Round Trip Time of Request-Reply}
  The observation seen in ~\autoref{fig:result-request-reply-rtt} was quite inconsistent. It is difficult to make any strong conclusions from the result. Nevertheless, it seems that the increase in number of suppliers decreases latency, as seen in case of eight suppliers against the single supplier.

  There could be many factors affecting the result. Even the slight change in network latency can induce a significant change in time required to transfer a message.

\section{The DDE Framework}
  According to the observations made after creating and testing the applications based on the framework (as discussed in \autoref{sec:result-framework}), we can infer the application responded well when the system was scaled up. We can see the rise in message throughput with higher number of isolate systems. The raise in message throughput was even more when number of Message Queuing Systems were increased.

  During the testing, restarting of message broker system (RabbitMQ) and restarting of message queuing systems did not make the whole system fail. The system operated normally as soon as they were back online. Thus, we can say that the system was highly available and it is suitable for the systems that need to be “run forever”.

\section{Problems/Issues}

\subsection{Dart Induced Issues}
  Even though Dart is a fairly mature language, it still has some bugs and incomplete features. Some of the issues could be solved by workarounds while others could not be easily resolved. All the issues listed here were faced here were in the Dart version 1.7.2.

  Dart's documentation for Isolate~\cite{dartApiIsolate} mentions that there is experimental support for following methods in isolate: \emph{addErrorListener},  \emph{addOnExitListener}, \emph{kill}, \emph{pause}, \emph{ping}, \emph{removeErrorListener}, \emph{removeOnExitListener}, \emph{resume}. But, while developing the framework, it was found that these methods were not implemented yet. The implementation of these functionalities would have particularly helped in the implementation of supervision strategy. Moreover, the implementation of features like ‘hot deployment’, ‘migration’ and isolate termination would have been cleaner and simpler compared to the current workaround present in the DDE framework.

  We have already discussed the workaround implementation for the \emph{kill} in \autoref{itm:killWorking} and \emph{ping} in \autoref{subsec:router}.

  Another issue found in the Dart which made it impossible for an isolate system designed in DDE framework to run in browser was the lack of support for multi-level isolates in browser. The ‘Dartium’ browser~\footnote{a variant of Chromium browser with built in dart VM}, only supports spawning of isolate up to single level deep. i.e. Spawning of isolate by the spawned isolate was not allowed. The reason behind the lack of this implementation is not clear.

\subsection{STOMP Library for Dart}
  As mentioned in \autoref{subsec:stompForDart}, a third-party library for STOMP client in Dart was used from Dart's ‘pub’. Although it had good support for stomp 1.2, it was missing a feature to set prefetch count~[\autoref{subsec:rabbitmqPrefetch}] while subscribing as consumer. Without setting prefetch-count the system became unusable especially, when there were lots of messages in the queue.

  As the project was hosted in git hub~\footnote{\url{http://github.com}} and open source and licensed under Apache 2.0~\footnote{\url{https://github.com/rikulo/stomp/blob/master/LICENSE}}, I forked~\footnote{make copy of the source code and add custom modifications} the repository and added the functionality so that the STOMP client could support setting the prefetch-count. After making modifications and testing, I created a pull-request~\footnote{request to merge my changes into the master branch of the original repository}, which was swiftly merged~\footnote{\url{https://github.com/rikulo/stomp/pull/15}} by the original developer and was updated in the dart's pub as a new version.
